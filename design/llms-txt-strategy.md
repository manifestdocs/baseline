# Creating and distributing llms.txt for a programming language

**The llms.txt standard gives LLMs structured access to your documentation at inference time, and the best implementations use tiered file sizes, topic segmentation, and platform distribution through services like Context7.** Created by Jeremy Howard of Answer.AI in September 2024, the standard has been adopted by major developer projects including Stripe, Cloudflare, Svelte, Angular, and Astro. While no LLM provider has officially confirmed automated crawling of llms.txt files, the standard has proven highly effective for AI coding assistants like Cursor, Windsurf, and Claude Code, where developers feed documentation context to LLMs directly. This guide covers the full specification, real-world patterns, hosting details, and distribution strategies for a programming language project.

## The llms.txt specification and how it works

The llms.txt standard is a **Markdown file served at `/llms.txt`** on a website's root path that provides LLM-friendly content — essentially a curated index of a project's documentation with links to detailed resources. Unlike robots.txt (which controls crawler access) or sitemap.xml (which lists indexable pages), llms.txt is designed specifically for **inference time** — when a user is actively seeking assistance, not for model training.

The specification lives at [llmstxt.org](https://llmstxt.org/) and defines a strict section ordering. Only one element is required: an **H1 heading with the project name**. Everything else is optional but follows a defined sequence:

1. **H1 title** (required): `# ProjectName`
2. **Blockquote summary** (optional): `> A concise description of the project`
3. **Body text** (optional): Paragraphs or lists (no headings) with additional context, caveats, or important notes
4. **H2 sections with file lists** (optional): Each section contains markdown links in the format `- [Link Title](url): Optional description`
5. **`## Optional` section** (special meaning): URLs here can be skipped when shorter context is needed

The spec explicitly uses Markdown rather than XML because LLMs parse it natively. All links can point to both internal and external resources, and the spec recommends that sites provide clean Markdown versions of pages by appending `.md` to URLs (e.g., `docs.example.com/syntax.html` → `docs.example.com/syntax.html.md`).

**llms-full.txt** is the companion file containing complete documentation in a single Markdown file, developed by Mintlify in collaboration with Anthropic. Traffic data from Profound (an answer engine optimization platform) shows AI agents visit llms-full.txt **over twice as often** as the index-style llms.txt. Two additional variants exist: **llms-ctx.txt** (llms.txt with all linked content expanded inline, excluding the Optional section) and **llms-ctx-full.txt** (same but including Optional content), generated by the official `llms_txt2ctx` CLI tool.

## How programming languages should structure their llms.txt

Analysis of the best real-world implementations reveals five structural patterns, three of which are especially relevant for programming language projects.

**The tiered approach** (used by Svelte and Astro) provides multiple file sizes — small, medium, and full — letting LLMs choose the appropriate context depth. Svelte's implementation is exemplary: its root llms.txt links to `llms-small.txt` (compressed, minimal examples), `llms-medium.txt` (abridged, examples removed), and `llms-full.txt` (complete), plus per-package files for Svelte, SvelteKit, CLI, and MCP. This is ideal for languages with extensive documentation that exceeds typical **128K–200K token** context windows.

**The topic-segmented approach** (used by Astro) stores separate files in a dedicated directory like `/_llms-txt/`, covering distinct areas: API reference, tutorials, deployment guides, migration guides, and recipes. This lets an LLM fetch only the relevant topic area, keeping token usage efficient. For a programming language, this translates to separate files for syntax reference, type system, standard library modules, and tutorials.

**The language-specific approach** (used by Supabase) provides per-programming-language reference files under `/llms/`, such as `js.txt`, `python.txt`, `dart.txt`. While designed for multi-language SDKs, this pattern is relevant for languages with bindings or FFI documentation in multiple host languages.

For a programming language project, the recommended content structure combines these patterns:

```markdown
# MyLang v3.2

> MyLang is a statically-typed, compiled language designed for systems
> programming with memory safety and zero-cost abstractions.

Important notes:
- MyLang 3.2 introduces the new `async` syntax; code from 3.1
  using the old coroutine API needs migration
- MyLang is NOT garbage collected; it uses ownership-based
  memory management

For complete documentation, see [llms-full.txt](https://mylang.org/llms-full.txt).

## Getting Started
- [Installation](https://mylang.org/docs/install.md): Platform-specific setup
- [Quick Start](https://mylang.org/docs/quickstart.md): First program in 5 minutes
- [By Example](https://mylang.org/docs/by-example.md): Annotated code examples

## Language Reference
- [Syntax Reference](https://mylang.org/docs/syntax.md): Grammar and syntax rules
- [Type System](https://mylang.org/docs/types.md): Generics, traits, inference
- [Ownership](https://mylang.org/docs/ownership.md): Memory management model
- [Error Handling](https://mylang.org/docs/errors.md): Result types and propagation
- [Async Programming](https://mylang.org/docs/async.md): Futures and concurrency

## Standard Library
- [Collections](https://mylang.org/stdlib/collections.md): Vec, HashMap, BTreeMap
- [IO](https://mylang.org/stdlib/io.md): File I/O, buffered readers/writers
- [Net](https://mylang.org/stdlib/net.md): TCP/UDP, HTTP client
- [Strings](https://mylang.org/stdlib/strings.md): Manipulation, regex, formatting

## Optional
- [Migration Guide](https://mylang.org/docs/migration-3.2.md): Breaking changes
- [Compiler Internals](https://mylang.org/docs/internals.md): Architecture
- [FFI Guide](https://mylang.org/docs/ffi.md): C interop
- [Changelog](https://mylang.org/changelog.md)
```

The "Important notes" section before the H2 headings deserves special attention — it's where you place **common misconceptions and version-specific caveats** that prevent LLMs from generating incorrect code. Angular's team, for instance, created their llms.txt specifically to stop LLMs from generating outdated NgModule-based syntax instead of modern standalone components.

## Serving llms.txt from your language's website

**URL placement** follows the robots.txt convention: serve the primary file at the domain root (`https://mylang.org/llms.txt`). The spec explicitly supports subpaths, so documentation subdomains and versioned paths work naturally: `docs.mylang.org/llms.txt`, `/docs/v3.2/llms.txt`. Stripe demonstrates the two-level approach effectively — `stripe.com/llms.txt` serves a product-level index while `docs.stripe.com/llms.txt` contains the technical documentation map.

For **content type**, serve as `text/plain; charset=utf-8` for maximum compatibility. UTF-8 encoding is required; avoid BOM markers. Standard gzip/brotli compression is fine. Some implementations add an HTML `<link>` tag to signal the file's existence: `<link rel="alternate" type="text/markdown" href="/llms.txt" />`.

**Size targets** matter significantly. The index-style llms.txt should stay under **10KB** (~50–200 linked resources). The llms-full.txt file ideally stays under **200K tokens**, though this is challenging for large language projects. Turborepo's 116,000-token llms.txt is cited as a cautionary example of what happens when index and full content are conflated. If your full documentation exceeds 200K tokens, provide topic-segmented files (following Astro's `/_llms-txt/` pattern) so LLMs can fetch only what they need.

**Versioning** should use subpath-based strategies. Maintain `/llms.txt` for the current/latest version, with version-specific files at paths like `/docs/v3.2/llms.txt`. Include the version number in the H1 heading and blockquote. Documentation platform Fern supports this natively with hierarchical llms.txt generation at every level of the docs.

**Caching** requires balancing freshness with performance. Use `Cache-Control: public, max-age=3600` (1 hour) for actively developed documentation, with longer TTLs for stable releases. Always include `ETag` or `Last-Modified` headers for conditional requests. The critical pitfall is CDN caching — deploy cache purge automation so updates propagate immediately after documentation changes.

The strongest practice is **automating generation as part of your documentation build pipeline**. Several tools support this directly:

- **VitePress**: `vitepress-plugin-llms` generates llms.txt, llms-full.txt, and per-page .md files with support for `<llm-only>` and `<llm-exclude>` tags
- **Docusaurus**: `docusaurus-plugin-llms` generates both files with glob-pattern ordering and content cleaning
- **MkDocs**: Dedicated plugin for llms.txt generation
- **Mintlify, GitBook, Fern**: Auto-generate on every publish
- **Rust-specific**: `rustdoc-llms` and `cargo-llms-txt` generate from Rust crate documentation
- **General-purpose**: `llms_txt2ctx` (official Python CLI from Answer.AI) parses and expands llms.txt into context files

## Getting indexed on Context7 and similar platforms

**Context7** (built by the Upstash team at [context7.com](https://context7.com)) is the leading platform for serving up-to-date, version-specific documentation to AI coding assistants. It solves the core problem that LLMs hallucinate APIs from stale training data by providing a **5-stage pipeline**: parse documentation from GitHub → enrich with LLM-generated metadata → vectorize into embeddings → rerank for relevance → cache via Redis. It integrates with **30+ AI tools** through an MCP server, REST API, and TypeScript SDK.

**Submitting a programming language to Context7** follows three paths. The fastest is the web interface at `context7.com/add-library?tab=github` — paste your GitHub repository URL containing documentation, optionally adjust folder inclusions, and submit. Processing takes **1–10 minutes**. For fine-grained control, place a `context7.json` in your repository root:

```json
{
  "$schema": "https://context7.com/schema/context7.json",
  "projectTitle": "MyLang",
  "description": "A systems programming language with memory safety",
  "branch": "main",
  "folders": ["docs", "stdlib-docs"],
  "excludeFolders": ["src", "i18n/zh*", "archived"],
  "excludeFiles": ["CHANGELOG.md"],
  "rules": ["Use MyLang 3.2 syntax", "Prefer ownership over references"],
  "previousVersions": [{ "tag": "v3.1.0" }, { "tag": "v3.0.0" }]
}
```

Library owners can **claim ownership** by adding a `public_key` verification field to context7.json, unlocking an admin panel for managing settings, team members, and versions. Context7 performs **daily re-indexing** automatically. Documentation must be in **md, mdx, txt, rst, or ipynb format** in a public GitHub repository (private repos require a Pro plan). The `rules` array is particularly valuable for programming languages — inject best practices like "Always use the v3.2 async syntax" that get included in every context response.

**Alternative platforms** worth considering alongside Context7:

| Platform | Differentiator | Cost |
|----------|---------------|------|
| **Docfork** | Open-source, 9000+ libraries, ~500ms delivery | Free |
| **Nia** (YC-backed) | Indexes codebases + docs, shared agent context | Free (100 req) / $14.99/mo |
| **Deepcon** | Claims 90% vs 65% accuracy in benchmarks | Free (100 req/mo) / $8–20/mo |
| **GitMCP** | Direct GitHub API access for code and docs | Free |
| **DeepWiki** | Auto-generates architecture diagrams from repos | Free (public repos) |

**llms.txt directories** provide additional discoverability: [llmstxthub.com](https://llmstxthub.com) (the largest directory), [directory.llmstxt.cloud](https://directory.llmstxt.cloud), and [llmsdirectory.com](https://llmsdirectory.com) all maintain curated indexes of sites implementing the standard. Submitting your language's llms.txt to these directories increases visibility.

## Patterns from the most effective real-world implementations

Studying the best implementations reveals actionable patterns that a programming language project should adopt.

**Svelte sets the gold standard** with its tiered system: `llms-small.txt` (minimal, examples removed), `llms-medium.txt` (abridged), `llms-full.txt` (complete), plus per-package llms.txt files for each sub-project. The root llms.txt serves purely as a router to these variants. Svelte also provides a dedicated documentation page at `/docs/llms` explaining the convention to users and contributors — a practice worth emulating.

**Stripe demonstrates enterprise-scale architecture** with two complementary files: `stripe.com/llms.txt` maps the product landscape while `docs.stripe.com/llms.txt` covers technical documentation. Every documentation page is available as `.md` by URL convention, and every page displays "LLM? Read llms.txt." Both files reference `.md` versions of all linked pages. This **two-level hierarchy** — marketing/overview at the company domain, technical depth at the docs subdomain — is the model for language projects that separate their main site from documentation.

**Astro's topic segmentation** is the most practical pattern for large documentation sets. Rather than one massive file, Astro stores topic-specific files in `/_llms-txt/`: `api-reference.txt`, `how-to-recipes.txt`, `build-a-blog-tutorial.txt`, `deployment-guides.txt`, `migration-guides.txt`. The root llms.txt links to these plus `llms-small.txt` and `llms-full.txt`. This approach maps directly to a programming language's natural documentation divisions — syntax reference, standard library, tutorials, and tooling guides.

**Angular's implementation** reveals the "why" behind adoption. The Angular team created their llms.txt specifically because LLMs kept generating **deprecated NgModule-based code** instead of modern standalone components. Their table-of-contents style llms.txt organizes links by concept (Components, Templates, Signals, DI, Routing, SSR) and they additionally provide an MCP server with 7+ tools. The lesson: **identify what LLMs get wrong about your language and structure llms.txt to correct those specific misconceptions**.

**Cloudflare** represents the upper bound of scale — their llms.txt covers **50+ products** with every link pointing to `.md` versions. For a programming language with a very large standard library, this product-per-section approach maps to module-per-section organization.

## Conclusion

The most effective llms.txt strategy for a programming language combines **four elements**: a concise root index file under 10KB with a blockquote summary and caveats section that corrects common LLM misconceptions; a **tiered file system** (small/medium/full variants) for different context window budgets; **topic-segmented files** for language reference, standard library, and tutorials; and **distribution through both your own website and context platforms** like Context7. Automate generation in your documentation build pipeline using platform-specific plugins (VitePress, Docusaurus, MkDocs) or the official `llms_txt2ctx` CLI. Register with Context7 via their web interface or `context7.json`, claim ownership for verified status, and submit to llms.txt directories for discoverability. The standard remains a proposal rather than an official web standard, but its practical value for AI coding assistants has driven adoption across the developer ecosystem — over **2,000 websites** now serve llms.txt files, and the trend is accelerating as AI-assisted development becomes the norm.
